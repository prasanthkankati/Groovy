*** Jmeter Tool UI***
jmeter-template: (pre configured test plan or a structure)adds a default structure to the testplan priorly so depend on the requirement we can choose the corresponding template. recording, recording with think time
jmeter-toggle- enable and diables the selected elements
jmeter start- start the testplan and executes
jmeter start-no puse - ignore all the timers and execute the tests without any pause.
jmeter stop- abruput ending- means-forcefully stop execution
jmeter shutdown button- gracefull ending.means- once the current thread execution completed then it would stop
jmeter clear button- clears the current element 
jmeter clear all button - clears all the elements and refreshes.
jmeter search- can search for elements
jmeter reset serach- erases search
jmeter function helper- can choose the function and generate the syntax and use from it
jmtere help- opens the jmeter documentation
-
Thread grop - using it we can define the virtual users(load), how long they run(duration),  we can use thread lifetime to define how long the load should be on hold.
	concurency thread group
  stepping threadgroup supports throupit shaping timer where we can define RPS nog the users. rampup tps, hold rps,rampdown rps.
***Jmeter-Listners***

Listners- shows the request and reponse info like payload and reponse with diffrent metrics depend on the listerner we choose 

view results tree listner(not recommonded only use for debugging consumes much resources)- shows the rquest body nd reponse body, response headers, load time, connect time, message, status code.

View results in table: sample#, start time, threadname, status, received bytes, sent bytes, error %, throughput.

Summary report:

View results in table: sample#, start time, threadname, status, received bytes, sent bytes, error %, throughput.

grapgh results: avarage, median, throughput, deviation.

simple data writer (for highload performance testing): if we upload the csv file in this listener it will append the results to it after the run.

assertion results listner: this listener shows the asserion results passed or failed

***

*** Jmeter-Assertions***

Assertion : it is check to valiadte the jmeter response

types: 

Response assertion: check the response message, status code, response text by comapring the actual and expected values.

Duration assertion: checks the response duartion by provideing expected value in the assertion config part

size assertion: checks the size in bytes and copare the expected size with actual
XML assertion: checks if the response parsed into well formated xml format

HTML assertion: jmeter response is formates using interner jmeter HTML parser so it checks for any errors or warnings exists in the HTML response parsed like broken tags, incompleted elemenents, structural errors. so we will provide threashhold error and warnings counts if actual error and warnings are more than expcted or provided it fails.

Xpath assertion: it checksfor the nodes in the response which matches the provided xpath we can also store the matchings into afile using this, it no matches assertion fails.

Json assertion:

defaultly wont be there so we have to add a plugin-json to write a json expression user.id.job = "engineer" if it matches it pass.

***Jmeter Variables***
Variable stores the values which are defined by user.
syntax: ${userName}
Types: 
1. Local variables: set under the thread level using config elements > user defined variables
2. Global variables: defined in the testplan level 
local variables override the global variables.



***Jmeter Functions***

functions are values that can populate fields of samplers or other elements

Syntax: ${__functionName(),(),()}
${_log"info"}
${__threadNum}
${__threadGroupName}
${__intSum(1,5) value}
${__time (dd MM YYYY)}
${__random(6,abcdefx)}
${__machineName}
${__machineIP}
${__StringToFile}
${__StringFromFile}

controll users rampup and down from the CLI jmeter -n -t "path to jmx" -l "results.jtl"-jusers=10 -jrampup=5 -loopcount=5 
${__p(users,1)}
${__p(rampup,1)}
${__p(rampdown,1)}


*** DistributedLoad Master-Slave***

Master will controll the work nodes. 
jmeter should be installed on all the machine and the version should be same
java should be installed in all the machines
all the machines should be in the same network
then > in the master machine jmeter properties remote machines and rmi section add the salve IP's and comment the current IP of the master.

once its done > start the server and give the command to connect slave IP> it shows conncted > open jmeter remote run the test plan or you can run the jmx by providing -r IP in the test run command.

***Basic Autherization***

testplan > threadgroup > sampler > config eklement > HTTP AUTHORIZATION MANGER > add base url , username, password, type basic 

***Thread***
Programm contain a process and it process includes threads.
multiple proceess includes multiple threads.
monotoring thread helath is required to identify bottlenecks.
if high cpu utilization or high elapsed time than expected we may take thread dump to analyse the thread status

usually thread status are: running / blocked / waiting if not then they might struck, stall on, deadlock, high cpu consumption, contention (multiple threads waiting for resources)

with 5 sec gap we have to collect the thread dump

ways to collect:
1. we can directly collect the thread dump if we have access to the app server.
using jstack(Jstack <PID> threaddump.txt ),visual VM
2. if we dont have access then infra team will install agents on the server and then the agent sends the thread health to monotoring server and using url we can get the detailed metrics in a dashboard.

we can do the thread dump analysis manually by observing the cpu utilization time and status or we can use the tools like fast thread by uploading the threaddump file in the tools.



***Metric definitions***


Vusers: Replica of a real user which mimics the real user behaviour.
Bottleneck : if a resource restrict to perform that issue is a bottleneck.
Scalability : scaling up or scaling down the capacity of the system whenever needed.
Latency: is the time taken to move the datapacket from source to destination.
Request Latency: time taken to reach the data from client to server.
Response latency: time taken to reach the data packets from server back to client.
Round trip latency: Request latency + Response latency.
Throughput: Number of trnscations/requests/data processed by system during the given time frame. it might be requests/sec (RPS) or KB/sec or transcations/sec (TPS) depends on the context.
Response time: time taken to receive the response after the rquest made. it includes time taken to send the reqest + processing by system + sending back to client says as round trip response time.
Average response time: total response time % number of samplers. 3 sampelers 1 sec 3sec 1sec % 3 =1.66 if it is low performance is high.
median response time: middle value before which 50% of the responses were collected and 50% were late to this value.
standard deviation: indicate variation in the response time. it should be close to the average median for good performance should not too low or high to the median line.

starndard deviation population = sqrt of i=1 to N (xi-mean)2%N 1+2+3/3=2 2-1=1sqare =1 > like tat 3-1sr=2 +1-1sqr%3=3/3=0

90% line ex:320ms: 90% of the sampler responses reached before 320ms and remining are received after 320ms
95% line: 95% of the sampler responses reached before 320ms and remining are received after 320ms
99% line: 99% of the sampler responses reached before 320ms and remining are received after 320ms
minimum response time: it is minimum time taken to get the rsponse 
maximum response time: it is the maximum time taken to get the response.
received KB, sent KB: kb received and sent.
Rampup time: gradually launches the users from 0 to its maximum users to mimic the real time load and to avoid spikes and crashes.
Rampdown: gradually push back the load to 0 to mimic the real time load.
steadystate: this is whre we can collect the metrics as the defined peak load holds for the given time.
Saturation: peak point at which the maximum utilization of a resource occurs.
memory utilaztaion: percentage of the memory consumption to process a request.
CPU utilization: percentage of CPU computing power cosumed to process a request.
Concurent users: multiple usres are using the system at the same time but sending diffrent requests/performing diffrent tasks.
Simultationious users: multiple users ar3e using the system at the same time and also sending the same requests or doinsame task.
Think time: simulated time delay between two consequetive actions or requests to mimic the real user behaviour.
Peak time: anticipated busiest time of the server.
Peak load: (TTP: time to peak) highest expected user load on a sarver



*** Performance testing life cycle or process: ***

it includes 11 steps:

1. Requiremt gathering and analysis

in this phase we identify the performance golas such as response time, throuhput time, resource utilization by refering the BRS.
we understand the application archituture and infrastruture.
analyse the expected user patterns/use cases and workload models no of users load, type of tests, gegraphic location using the app.

2. planning and design

in this phase we define testing scope which is under scope which is out of scope.
objectives of testing and test strategy
testing approach we will be follwing
define the workload models and environment setup based on user behaviour.
identifying the risk, timeline, resources required.

3. Environment setup:

we make sure comparing the PROD and Testbeds which should as match as close to PROD environment.
necessary hardware and software and network we mirror with PROD env.

4. Tool selection

depend on the sills of resources
scope
extension
open source depend on these we choose tool

5. Test design
untit test
integration test
whole scenario testing
and verifying the test accuracy
test script generation depend on the real usage patters and senarios.
oprimiize the script using parametrzation and corelation.

6. Test execution

we do execute the scripts

7. Monitoring and analyis 

collect and monitor the KPI's such as response time, avarage time, resource utilization and objectives as part of SLA
identifying the bottle necks

8. Tuning & Optimization

Dev gives fix to tune apps

9. Retest:

we perform retest to verify the fix 
we also perform regression to check the overall helath of existing system

10. Reports:

we generate and create reports

11. Continous monotoring

we do moniotor the helath and report if any issues found

***
how to read a large json body for a POST, PUT, PATCH requests.

we create the json external file and keep the json bosy into it.
in the request body using ${__FileToString(pathof the file),(encryptionoptional),(variable optional)}
we provide the fuction then it will read from the file through function.

***
while doing Load test with 4K threads within 1hr we hold for 66 users per min instead of finishing the 66 users in seconds?

we use arrival thread group to achieve this

Testpalan > arrival thread group > arrivals/min or targetrate/min = 66 , Rampup = 1min, Rampup step count= 10, Hold traget rate time in min =60

it will launch 66 users per min and it holds for 1 hr.

***
how to corelate session ID or customer token ID while authenticating?

from one of the login request response we will see session id generated > we store the ID by writing a regexpression session_ID="(.*?)" > token as variable name so > from the next post requests we use the ${token} variable to post and use the same session ID stored cause it is dynamic in nature every logout it will change

***

How to load ramp up period of 60 seconds, and steady state load for 60 sec  nd rampdown 60 seconds and filter only the steadystte results workloadmodel?

jmeter plugins > install filterresults plugin.


Testplan > ultimate thread group > thread count 10 , Startup time 60 sec, shtdowntime 60 sec, hold load for 60 sec > results tree > save jmx run in cl mode

jmeter -n -t "path to the jmx" -l "path to the results.jtl"

results.jtl will be generated > now, we should keep the terminal directory in to jmeter bin and run the follwing command to trim the first 60 sec and last 60 sec from results.

filterresults.bat --output-file"path/filteredresults.csv" --input-file"path/results.jtl" --startoffset-60 --endoffset-60

we have to make sure the cmdrunnerjar version should be same in the filterresults.bat file otherwise it throws error.

like the same way we can filter patterns like specific http requests using reg expression in the same command including.


*** monitor server metrics using perfmon plugin ***

1. install pluginmanager if not 
2. install perfmon plugin in jmeter
3. install serverAgent on the target device and start the agent before running the jmater test so that it listen the port 4444
4. add perfmon metrics collector lister which shows only when plugin added in jmeter
5. in the listner add IP of the traget device and port 4444 add metrics like CPU, memory, network, disk I/o 
6. run the listener so it collects the server metrics we can compare with native taskmanager to check and then export.
7, we can also combine both jmeter resukts and sever metrics performon collector results and generate report unsing influx db + gafana






